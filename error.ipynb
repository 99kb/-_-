{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비화재보 분류 \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 년도별 데이터 결합 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
     ]
    }
   ],
   "source": [
    "start_year= 2019\n",
    "end_year =2021\n",
    "\n",
    "df_all =pd.DataFrame()\n",
    "#년도합치기\n",
    "for year in range(start_year,end_year+1):\n",
    "    globals()[f'df_{year}']=pd.DataFrame()\n",
    "#Sheet 합치기\n",
    "    for page in range(1,10):\n",
    "        tmp = pd.read_excel(f\"{year}년 전체 신고건수_관할 및 재난주소 추가.xls\",sheet_name = f'Sheet {page}')['신고내용'].dropna(axis=0).T\n",
    "        globals()[f'df_{year}']= pd.concat([globals()[f'df_{year}'],tmp], axis = 0)\n",
    "    df_all= pd.concat([df_all,globals()[f'df_{year}']], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중복 문자열들 제거-set 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "949260\n",
      "                                                   0\n",
      "0  [구급상황센터 이관]5살 아기 / 문에 손가락을 찡김 / 손가락 깊이 파임 / 의료...\n",
      "1                        [구급상황센터 이관]북구 침산동 / 병원안내 / \n",
      "2                   지산 2단지 202동 1104호 / 할머니 넘어지짐 위중/\n",
      "3   매호화성파크드림 101동 508호/남편이 어제부터 상태가 좋지 않다/구토심하고 거동불가\n",
      "4                                  칠곡경북대 금호 베드 회수 요청\n",
      "778936\n"
     ]
    }
   ],
   "source": [
    "print(len(df_all))\n",
    "df_all= pd.DataFrame(set(df_all[0]))\n",
    "print(df_all.head())\n",
    "print(len(df_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  신고내용 중 숫자와 영어같은 비화재보 분류와 관련없는 단어들 제거 &전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('속', 'Modifier'), ('보기', 'Noun')]\n",
      "[('경보기', 'Noun')]\n",
      "[('경보', 'Noun')]\n",
      "[('속보', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt \n",
    "okt = Okt()\n",
    "print(okt.pos('속보기')) \n",
    "#속보기 단어인식을 못함 \n",
    "#한단어로 인식 X\n",
    "print(okt.pos('경보기')) \n",
    "print(okt.pos('경보')) \n",
    "print(okt.pos('속보')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0   구급상황센터 이관  살 아기   문에 손가락을 찡김   손가락 깊이 파임   의료...\n",
      "1                         구급상황센터 이관 북구 침산동   병원안내   \n",
      "2                   지산  단지    동     호   할머니 넘어지짐 위중 \n",
      "3   매호화성파크드림    동    호 남편이 어제부터 상태가 좋지 않다 구토심하고 거동불가\n",
      "4                                  칠곡경북대 금호 베드 회수 요청\n"
     ]
    }
   ],
   "source": [
    "df_all2 = df_all.copy()\n",
    "for i in range(len(df_all2)):\n",
    "    tmp = re.sub(r'[^가-힣+ ]',' ',df_all2.iloc[i,0]) # 한글말고 안나오게\n",
    "    tmp = re.sub('속보기','속보',tmp)\n",
    "#속보기 ->속보로 단어 치환    \n",
    "    df_all2.iloc[i,0] = tmp\n",
    "print(df_all2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5996\n",
      "1\n",
      "                                   0\n",
      "751563  속보기 앞산        추가 오작동 확인 출동취소\n"
     ]
    }
   ],
   "source": [
    "#속보기 단어 제거 확인\n",
    "print(df_all[0].apply(lambda x: re.search(\"속보기\", x)is not None).sum())\n",
    "print(df_all2[0].apply(lambda x: re.search(\"속보기\", x)is not None).sum())\n",
    "print(df_all2[df_all2[0].apply(lambda x: re.search(\"속보기\", x)is not None)])\n",
    "### 왜 1개 제거가 안됏는지?\n",
    "# r'[^가      r의 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  문장 -> 형태소 형태로 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tokenizing]  778936  /  778936   100.0  %                                                                                                                                                                     "
     ]
    }
   ],
   "source": [
    "# 형태소 분석(Noun, Adjective) -명사와 형용사만\n",
    "# 1자를 배제\n",
    "# 불용어 제외\n",
    "sents = []\n",
    "for i in range(len(df_all2[0])):\n",
    "    print(\"\\r[tokenizing]  {}  /  {}   {}  %\".format(i+1, len(df_all2[0]), round( (i / len(df_all2[0])) * 100 , 1 ) ), end = '    ', flush = False)\n",
    "    pos_res = okt.pos(df_all2[0].iloc[i])\n",
    "    \n",
    "    STOP_WORDS = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "    keywords = []\n",
    "    for word, pos in pos_res:\n",
    "        if ((pos == \"Noun\")|(pos == \"Adjective\"))&(len(word) >= 2)&(word not in STOP_WORDS):\n",
    "            keywords.append(word)\n",
    "    sents.append(' '.join(keywords))\n",
    "print(sents[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경보설비, 속보 설비   //      테스트, 오인주의, 훈련, 점검 문의\"  키워드/갯수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11027"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all2[0].apply(lambda x: re.search(\"속보\", x)is not None).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11021\n"
     ]
    }
   ],
   "source": [
    "check = []\n",
    "for i in range(len(sents)) :\n",
    "    if '속보' in sents[i]:\n",
    "            check.append(sents[i])\n",
    "print(len(check))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all2[0].apply(lambda x: re.search(\"경보\", x)is not None).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8108\n"
     ]
    }
   ],
   "source": [
    "check = []\n",
    "for i in range(len(sents)) :\n",
    "    if '경보' in sents[i]:\n",
    "            check.append(sents[i])\n",
    "print(len(check))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시저장\n",
    "df_sents = pd.DataFrame(sents)\n",
    "df_sents.to_csv(\"임시저장.csv\",encoding= 'euc-kr',index= None)\n",
    "df_sents= pd.read_csv(\"임시저장.csv\",encoding= 'euc-kr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer Vectorizing\n",
    "랜덤포레스트?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778936, 54861)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "cv_sents=vect.fit_transform(df_sents[0])\n",
    "print(cv_sents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizing\n",
    "로지스틱 회귀?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778936, 54861)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer(min_df=0.0, analyzer='char', sublinear_tf=True, ngram_range=(1,3), max_features=5000)\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_sents =vectorizer.fit_transform(df_sents[0])\n",
    "print(tfidf_sents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Woed2vec(CBOW)\n",
    "로지스틱 회귀?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentences = []\n",
    "for review in df_sents[0]:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터\n",
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3 # 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 14:31:00,645 : INFO : collecting all words and their counts\n",
      "2022-01-20 14:31:00,646 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-01-20 14:31:00,662 : INFO : PROGRESS: at sentence #10000, processed 91888 words, keeping 7655 word types\n",
      "2022-01-20 14:31:00,682 : INFO : PROGRESS: at sentence #20000, processed 182808 words, keeping 10892 word types\n",
      "2022-01-20 14:31:00,700 : INFO : PROGRESS: at sentence #30000, processed 273587 words, keeping 13251 word types\n",
      "2022-01-20 14:31:00,720 : INFO : PROGRESS: at sentence #40000, processed 364357 words, keeping 15118 word types\n",
      "2022-01-20 14:31:00,741 : INFO : PROGRESS: at sentence #50000, processed 454946 words, keeping 16795 word types\n",
      "2022-01-20 14:31:00,768 : INFO : PROGRESS: at sentence #60000, processed 547104 words, keeping 18294 word types\n",
      "2022-01-20 14:31:00,783 : INFO : PROGRESS: at sentence #70000, processed 639852 words, keeping 19613 word types\n",
      "2022-01-20 14:31:00,800 : INFO : PROGRESS: at sentence #80000, processed 730109 words, keeping 20794 word types\n",
      "2022-01-20 14:31:00,818 : INFO : PROGRESS: at sentence #90000, processed 819890 words, keeping 21905 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-20 14:31:00,830 : INFO : PROGRESS: at sentence #100000, processed 910752 words, keeping 22997 word types\n",
      "2022-01-20 14:31:00,846 : INFO : PROGRESS: at sentence #110000, processed 1001710 words, keeping 24024 word types\n",
      "2022-01-20 14:31:00,865 : INFO : PROGRESS: at sentence #120000, processed 1092306 words, keeping 24981 word types\n",
      "2022-01-20 14:31:00,879 : INFO : PROGRESS: at sentence #130000, processed 1182461 words, keeping 25860 word types\n",
      "2022-01-20 14:31:00,892 : INFO : PROGRESS: at sentence #140000, processed 1272269 words, keeping 26744 word types\n",
      "2022-01-20 14:31:00,908 : INFO : PROGRESS: at sentence #150000, processed 1363019 words, keeping 27581 word types\n",
      "2022-01-20 14:31:00,930 : INFO : PROGRESS: at sentence #160000, processed 1453211 words, keeping 28367 word types\n",
      "2022-01-20 14:31:00,943 : INFO : PROGRESS: at sentence #170000, processed 1544012 words, keeping 29161 word types\n",
      "2022-01-20 14:31:00,959 : INFO : PROGRESS: at sentence #180000, processed 1636128 words, keeping 29927 word types\n",
      "2022-01-20 14:31:00,988 : INFO : PROGRESS: at sentence #190000, processed 1726668 words, keeping 30650 word types\n",
      "2022-01-20 14:31:01,007 : INFO : PROGRESS: at sentence #200000, processed 1816139 words, keeping 31288 word types\n",
      "2022-01-20 14:31:01,033 : INFO : PROGRESS: at sentence #210000, processed 1905568 words, keeping 31916 word types\n",
      "2022-01-20 14:31:01,048 : INFO : PROGRESS: at sentence #220000, processed 1996019 words, keeping 32581 word types\n",
      "2022-01-20 14:31:01,076 : INFO : PROGRESS: at sentence #230000, processed 2085702 words, keeping 33262 word types\n",
      "2022-01-20 14:31:01,107 : INFO : PROGRESS: at sentence #240000, processed 2176804 words, keeping 33889 word types\n",
      "2022-01-20 14:31:01,141 : INFO : PROGRESS: at sentence #250000, processed 2268798 words, keeping 34522 word types\n",
      "2022-01-20 14:31:01,157 : INFO : PROGRESS: at sentence #260000, processed 2359536 words, keeping 35127 word types\n",
      "2022-01-20 14:31:01,178 : INFO : PROGRESS: at sentence #270000, processed 2449821 words, keeping 35708 word types\n",
      "2022-01-20 14:31:01,191 : INFO : PROGRESS: at sentence #280000, processed 2540077 words, keeping 36254 word types\n",
      "2022-01-20 14:31:01,212 : INFO : PROGRESS: at sentence #290000, processed 2630455 words, keeping 36828 word types\n",
      "2022-01-20 14:31:01,225 : INFO : PROGRESS: at sentence #300000, processed 2722064 words, keeping 37293 word types\n",
      "2022-01-20 14:31:01,253 : INFO : PROGRESS: at sentence #310000, processed 2812231 words, keeping 37800 word types\n",
      "2022-01-20 14:31:01,271 : INFO : PROGRESS: at sentence #320000, processed 2902928 words, keeping 38335 word types\n",
      "2022-01-20 14:31:01,289 : INFO : PROGRESS: at sentence #330000, processed 2994382 words, keeping 38856 word types\n",
      "2022-01-20 14:31:01,310 : INFO : PROGRESS: at sentence #340000, processed 3084962 words, keeping 39346 word types\n",
      "2022-01-20 14:31:01,327 : INFO : PROGRESS: at sentence #350000, processed 3175993 words, keeping 39840 word types\n",
      "2022-01-20 14:31:01,342 : INFO : PROGRESS: at sentence #360000, processed 3268562 words, keeping 40343 word types\n",
      "2022-01-20 14:31:01,357 : INFO : PROGRESS: at sentence #370000, processed 3359591 words, keeping 40835 word types\n",
      "2022-01-20 14:31:01,382 : INFO : PROGRESS: at sentence #380000, processed 3451382 words, keeping 41290 word types\n",
      "2022-01-20 14:31:01,401 : INFO : PROGRESS: at sentence #390000, processed 3542759 words, keeping 41734 word types\n",
      "2022-01-20 14:31:01,418 : INFO : PROGRESS: at sentence #400000, processed 3634172 words, keeping 42207 word types\n",
      "2022-01-20 14:31:01,434 : INFO : PROGRESS: at sentence #410000, processed 3725447 words, keeping 42605 word types\n",
      "2022-01-20 14:31:01,450 : INFO : PROGRESS: at sentence #420000, processed 3815733 words, keeping 42998 word types\n",
      "2022-01-20 14:31:01,470 : INFO : PROGRESS: at sentence #430000, processed 3905552 words, keeping 43408 word types\n",
      "2022-01-20 14:31:01,493 : INFO : PROGRESS: at sentence #440000, processed 3998481 words, keeping 43850 word types\n",
      "2022-01-20 14:31:01,510 : INFO : PROGRESS: at sentence #450000, processed 4089422 words, keeping 44267 word types\n",
      "2022-01-20 14:31:01,527 : INFO : PROGRESS: at sentence #460000, processed 4180133 words, keeping 44678 word types\n",
      "2022-01-20 14:31:01,543 : INFO : PROGRESS: at sentence #470000, processed 4270665 words, keeping 45098 word types\n",
      "2022-01-20 14:31:01,561 : INFO : PROGRESS: at sentence #480000, processed 4361660 words, keeping 45475 word types\n",
      "2022-01-20 14:31:01,583 : INFO : PROGRESS: at sentence #490000, processed 4452709 words, keeping 45835 word types\n",
      "2022-01-20 14:31:01,606 : INFO : PROGRESS: at sentence #500000, processed 4543940 words, keeping 46216 word types\n",
      "2022-01-20 14:31:01,624 : INFO : PROGRESS: at sentence #510000, processed 4633519 words, keeping 46559 word types\n",
      "2022-01-20 14:31:01,640 : INFO : PROGRESS: at sentence #520000, processed 4722754 words, keeping 46905 word types\n",
      "2022-01-20 14:31:01,663 : INFO : PROGRESS: at sentence #530000, processed 4814156 words, keeping 47246 word types\n",
      "2022-01-20 14:31:01,682 : INFO : PROGRESS: at sentence #540000, processed 4905233 words, keeping 47585 word types\n",
      "2022-01-20 14:31:01,703 : INFO : PROGRESS: at sentence #550000, processed 4996716 words, keeping 47911 word types\n",
      "2022-01-20 14:31:01,717 : INFO : PROGRESS: at sentence #560000, processed 5088105 words, keeping 48237 word types\n",
      "2022-01-20 14:31:01,745 : INFO : PROGRESS: at sentence #570000, processed 5179027 words, keeping 48575 word types\n",
      "2022-01-20 14:31:01,764 : INFO : PROGRESS: at sentence #580000, processed 5269297 words, keeping 48927 word types\n",
      "2022-01-20 14:31:01,785 : INFO : PROGRESS: at sentence #590000, processed 5360080 words, keeping 49279 word types\n",
      "2022-01-20 14:31:01,806 : INFO : PROGRESS: at sentence #600000, processed 5450716 words, keeping 49617 word types\n",
      "2022-01-20 14:31:01,831 : INFO : PROGRESS: at sentence #610000, processed 5541095 words, keeping 49906 word types\n",
      "2022-01-20 14:31:01,858 : INFO : PROGRESS: at sentence #620000, processed 5632282 words, keeping 50224 word types\n",
      "2022-01-20 14:31:01,874 : INFO : PROGRESS: at sentence #630000, processed 5722674 words, keeping 50527 word types\n",
      "2022-01-20 14:31:01,899 : INFO : PROGRESS: at sentence #640000, processed 5812695 words, keeping 50807 word types\n",
      "2022-01-20 14:31:01,923 : INFO : PROGRESS: at sentence #650000, processed 5903104 words, keeping 51112 word types\n",
      "2022-01-20 14:31:01,944 : INFO : PROGRESS: at sentence #660000, processed 5993862 words, keeping 51422 word types\n",
      "2022-01-20 14:31:01,969 : INFO : PROGRESS: at sentence #670000, processed 6084156 words, keeping 51757 word types\n",
      "2022-01-20 14:31:01,986 : INFO : PROGRESS: at sentence #680000, processed 6174525 words, keeping 52032 word types\n",
      "2022-01-20 14:31:02,003 : INFO : PROGRESS: at sentence #690000, processed 6265461 words, keeping 52330 word types\n",
      "2022-01-20 14:31:02,018 : INFO : PROGRESS: at sentence #700000, processed 6354276 words, keeping 52623 word types\n",
      "2022-01-20 14:31:02,031 : INFO : PROGRESS: at sentence #710000, processed 6444960 words, keeping 52926 word types\n",
      "2022-01-20 14:31:02,054 : INFO : PROGRESS: at sentence #720000, processed 6534178 words, keeping 53199 word types\n",
      "2022-01-20 14:31:02,074 : INFO : PROGRESS: at sentence #730000, processed 6623751 words, keeping 53461 word types\n",
      "2022-01-20 14:31:02,088 : INFO : PROGRESS: at sentence #740000, processed 6714727 words, keeping 53763 word types\n",
      "2022-01-20 14:31:02,111 : INFO : PROGRESS: at sentence #750000, processed 6806241 words, keeping 54060 word types\n",
      "2022-01-20 14:31:02,124 : INFO : PROGRESS: at sentence #760000, processed 6896767 words, keeping 54346 word types\n",
      "2022-01-20 14:31:02,139 : INFO : PROGRESS: at sentence #770000, processed 6986964 words, keeping 54591 word types\n",
      "2022-01-20 14:31:02,160 : INFO : collected 54861 word types from a corpus of 7068489 raw words and 778936 sentences\n",
      "2022-01-20 14:31:02,162 : INFO : Loading a fresh vocabulary\n",
      "2022-01-20 14:31:02,609 : INFO : effective_min_count=40 retains 6447 unique words (11% of original 54861, drops 48414)\n",
      "2022-01-20 14:31:02,610 : INFO : effective_min_count=40 leaves 6836681 word corpus (96% of original 7068489, drops 231808)\n",
      "2022-01-20 14:31:02,628 : INFO : deleting the raw counts dictionary of 54861 items\n",
      "2022-01-20 14:31:02,630 : INFO : sample=0.001 downsamples 59 most-common words\n",
      "2022-01-20 14:31:02,630 : INFO : downsampling leaves estimated 5524995 word corpus (80.8% of prior 6836681)\n",
      "2022-01-20 14:31:02,643 : INFO : estimated required memory for 6447 words and 300 dimensions: 18696300 bytes\n",
      "2022-01-20 14:31:02,643 : INFO : resetting layer weights\n",
      "2022-01-20 14:31:03,799 : INFO : training model with 4 workers on 6447 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2022-01-20 14:31:04,826 : INFO : EPOCH 1 - PROGRESS: at 19.20% examples, 1047161 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:05,832 : INFO : EPOCH 1 - PROGRESS: at 37.19% examples, 1017099 words/s, in_qsize 8, out_qsize 0\n",
      "2022-01-20 14:31:06,837 : INFO : EPOCH 1 - PROGRESS: at 53.80% examples, 983726 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:07,840 : INFO : EPOCH 1 - PROGRESS: at 72.28% examples, 992798 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:08,842 : INFO : EPOCH 1 - PROGRESS: at 91.58% examples, 1005934 words/s, in_qsize 8, out_qsize 0\n",
      "2022-01-20 14:31:09,237 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-01-20 14:31:09,238 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-01-20 14:31:09,241 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-01-20 14:31:09,245 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-01-20 14:31:09,245 : INFO : EPOCH - 1 : training on 7068489 raw words (5524431 effective words) took 5.4s, 1016669 effective words/s\n",
      "2022-01-20 14:31:10,258 : INFO : EPOCH 2 - PROGRESS: at 17.93% examples, 991456 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:11,264 : INFO : EPOCH 2 - PROGRESS: at 37.06% examples, 1020328 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:12,264 : INFO : EPOCH 2 - PROGRESS: at 56.46% examples, 1039373 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:13,265 : INFO : EPOCH 2 - PROGRESS: at 74.55% examples, 1029012 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:14,269 : INFO : EPOCH 2 - PROGRESS: at 94.02% examples, 1036626 words/s, in_qsize 8, out_qsize 0\n",
      "2022-01-20 14:31:14,533 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-01-20 14:31:14,534 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-01-20 14:31:14,535 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-01-20 14:31:14,542 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-01-20 14:31:14,542 : INFO : EPOCH - 2 : training on 7068489 raw words (5524908 effective words) took 5.3s, 1045555 effective words/s\n",
      "2022-01-20 14:31:15,557 : INFO : EPOCH 3 - PROGRESS: at 19.48% examples, 1076605 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:16,560 : INFO : EPOCH 3 - PROGRESS: at 39.60% examples, 1091209 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:17,570 : INFO : EPOCH 3 - PROGRESS: at 59.57% examples, 1093532 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:18,575 : INFO : EPOCH 3 - PROGRESS: at 80.78% examples, 1111526 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:19,520 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-01-20 14:31:19,524 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-01-20 14:31:19,525 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-01-20 14:31:19,530 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-01-20 14:31:19,530 : INFO : EPOCH - 3 : training on 7068489 raw words (5524311 effective words) took 5.0s, 1110697 effective words/s\n",
      "2022-01-20 14:31:20,555 : INFO : EPOCH 4 - PROGRESS: at 19.76% examples, 1085422 words/s, in_qsize 8, out_qsize 0\n",
      "2022-01-20 14:31:21,555 : INFO : EPOCH 4 - PROGRESS: at 39.17% examples, 1077570 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:22,560 : INFO : EPOCH 4 - PROGRESS: at 59.43% examples, 1091855 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:23,568 : INFO : EPOCH 4 - PROGRESS: at 80.07% examples, 1101536 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:24,570 : INFO : EPOCH 4 - PROGRESS: at 97.11% examples, 1068410 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:24,685 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-01-20 14:31:24,686 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-01-20 14:31:24,693 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-01-20 14:31:24,696 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-01-20 14:31:24,697 : INFO : EPOCH - 4 : training on 7068489 raw words (5524596 effective words) took 5.1s, 1072880 effective words/s\n",
      "2022-01-20 14:31:25,710 : INFO : EPOCH 5 - PROGRESS: at 17.78% examples, 979284 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:26,711 : INFO : EPOCH 5 - PROGRESS: at 41.85% examples, 1153102 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:27,716 : INFO : EPOCH 5 - PROGRESS: at 65.52% examples, 1204177 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:28,717 : INFO : EPOCH 5 - PROGRESS: at 88.72% examples, 1222630 words/s, in_qsize 7, out_qsize 0\n",
      "2022-01-20 14:31:29,227 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2022-01-20 14:31:29,230 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-01-20 14:31:29,241 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-01-20 14:31:29,243 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-01-20 14:31:29,244 : INFO : EPOCH - 5 : training on 7068489 raw words (5525354 effective words) took 4.5s, 1217383 effective words/s\n",
      "2022-01-20 14:31:29,244 : INFO : training on a 35342445 raw words (27623600 effective words) took 25.4s, 1085615 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "print(\"Training model ....\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words,model, num_features):\n",
    "    feature_vector = np.zeros((num_features), dtype=np.float32)\n",
    "    num_words = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words +=1\n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(sentence, model, num_features):\n",
    "    dataset = list()\n",
    "    for s in sentence:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "    FeatureVecs = np.stack(dataset)\n",
    "    return FeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_12864/1152472743.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  feature_vector = np.add(feature_vector, model[w])\n",
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_12864/1152472743.py:10: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vector = np.divide(feature_vector, num_words)\n"
     ]
    }
   ],
   "source": [
    "train_data_vecs = get_dataset(sentences, model, num_features)\n",
    "print(train_data_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
