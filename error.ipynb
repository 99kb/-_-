{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비화재보 분류 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. 데이터 전처리     \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 년도별 데이터 결합 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re\n",
    "import pickle\n",
    "import xlrd\n",
    "start_year= 2016\n",
    "end_year =2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_all =pd.DataFrame()\n",
    "#년도합치기\n",
    "for year in range(start_year,end_year+1):\n",
    "#시트 이름 가져오기    \n",
    "    wb = xlrd.open_workbook(f\"{year}년 전체 신고건수_관할 및 재난주소 추가.xls\")\n",
    "    globals()[f'df_{year}']=pd.DataFrame()\n",
    "#Sheet 합치기\n",
    "    for  i in range(len(wb.sheets())):\n",
    "        tmp = pd.read_excel(f\"{year}년 전체 신고건수_관할 및 재난주소 추가.xls\",sheet_name = wb.sheet_names()[i])['신고내용'].dropna(axis=0).T\n",
    "        globals()[f'df_{year}']= pd.concat([globals()[f'df_{year}'],tmp], axis = 0)\n",
    "    df_all= pd.concat([df_all,globals()[f'df_{year}']], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"119_신고내용_{start_year} - {end_year}.pickle\",\"wb\") as fw:\n",
    "#     pickle.dump(df_all, fw)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"119_신고내용_{start_year} - {end_year}.pickle\",'rb') as fr:\n",
    "    df_all = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중복 문자열 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1711724\n",
      "1368100\n",
      "예시\n",
      "                                                   0\n",
      "0                     태전동 롯데아파트 1동 508호/심부전증 환자 거동불가\n",
      "1  동구 화랑로35길 58-5 / 재활용 가게 / 난로때문에 화재가 있었다 /// 연기...\n",
      "2                                     구암교회 뒷산쪽, 멧돼지 \n"
     ]
    }
   ],
   "source": [
    "print(len(df_all))\n",
    "df_all= pd.DataFrame(set(df_all[0]))\n",
    "print(len(df_all))\n",
    "print('예시')\n",
    "print(df_all.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  신고내용 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('속', 'Modifier'), ('보기', 'Noun')]\n",
      "[('경보기', 'Noun')]\n",
      "[('경보', 'Noun')]\n",
      "[('속보', 'Noun')]\n",
      "[('경', 'Modifier'), ('보가', 'Noun'), ('울', 'Modifier'), ('린다', 'Noun')]\n",
      "[('경보', 'Noun'), ('가', 'Josa'), ('울', 'Modifier'), ('린다', 'Noun')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt \n",
    "okt = Okt()\n",
    "print(okt.pos('속보기')) \n",
    "#속보기 단어인식을 못함 \n",
    "#한단어로 인식 X\n",
    "print(okt.pos('경보가울림'))\n",
    "# 경 보가  로 판단됨(띄어쓰기를 제대로 하지않았을때)\n",
    "print(okt.pos('경보기')) \n",
    "print(okt.pos('경보')) \n",
    "print(okt.pos('속보')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0                     태전동 롯데아파트  동    호 심부전증 환자 거동불가\n",
      "1  동구 화랑로  길        재활용 가게   난로때문에 화재가 있었다     연기...\n",
      "2                                     구암교회 뒷산쪽  멧돼지 \n",
      "1368100\n"
     ]
    }
   ],
   "source": [
    "df_all2 = df_all.copy()\n",
    "for i in range(len(df_all2)):\n",
    "    tmp = re.sub(r'[^가-힣+ ]',' ',df_all2.iloc[i,0]) # 한글말고 제거\n",
    "    tmp = re.sub('속보기','속보',tmp) # 속보기 ->속보로 단어 치환    \n",
    "    df_all2.iloc[i,0] = tmp\n",
    "print(df_all2.head(3))\n",
    "print(len(df_all2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "속보기 단어 수   : 7937\n",
      "속보기 단어 제거 :1\n",
      "제거 안된 문장 :702307    속보기 앞산        추가 오작동 확인 출동취소\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#속보기 단어 제거 확인\n",
    "print(f'속보기 단어 수   : {df_all[0].apply(lambda x: re.search(\"속보기\", x)is not None).sum()}')\n",
    "print(f'속보기 단어 제거 :{df_all2[0].apply(lambda x: re.search(\"속보기\", x)is not None).sum()}')\n",
    "print(f'제거 안된 문장 :{df_all2[0][df_all2[0].apply(lambda x: re.search(\"속보기\", x)is not None)]}')\n",
    "### 왜 1개 제거가 안됏는지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 575070 \n",
    "# tmp = re.sub(r'[^가-힣+ ]',' ',df_all2.iloc[i,0]) # 한글말고 제거\n",
    "# tmp = re.sub('속보기','속보',tmp)\n",
    "# print(tmp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  문장 -> 형태소 형태로 변환\n",
    "형태소 분석(Noun, Adjective) -명사와 형용사만  \n",
    "한글자를 배제  \n",
    "불용어 제외  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tokenizing]  1368100  /  1368100   100.0  %    ['태전동 롯데 아파트 심부전증 환자 거동 불가', '동구 화랑로 재활용 가게 난로 때문 화재 있었다 연기 진화 완료 현장 지휘 구급차 요청 사후 조사', '교회 뒷산 멧돼지', '달성 공원 대신 여인숙 할머니 온몸 통증 호소', '리치 여성 우울증 본동 주영 본동']                                                                                                                                                                                                                                                                                                                                                          \n"
     ]
    }
   ],
   "source": [
    "\n",
    "sents = []\n",
    "for i in range(len(df_all2[0])):\n",
    "    print(\"\\r[tokenizing]  {}  /  {}   {}  %\".format(i+1, len(df_all2[0]), round( (i / len(df_all2[0])) * 100 , 1 ) ), end = '    ', flush = False)\n",
    "    pos_res = okt.pos(df_all2[0].iloc[i])\n",
    "    \n",
    "    STOP_WORDS = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "    keywords = []\n",
    "    for word, pos in pos_res:\n",
    "        if ((pos == \"Noun\")|(pos == \"Adjective\"))&(len(word) >= 2)&(word not in STOP_WORDS):\n",
    "            keywords.append(word)\n",
    "    sents.append(' '.join(keywords))\n",
    "print(sents[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  전처리 전후 키워드 갯수 확인\n",
    "비화재보 주요 키워드 : 경보, 속보  \n",
    "비화재보 제외 키워드 : 테스트, 오인주의, 훈련, 점검   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 경보, 속보  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 [속보] 단어 수: 16048\n",
      "전처리 후 [속보] 단어 수: 16038\n"
     ]
    }
   ],
   "source": [
    "check = []\n",
    "for i in range(len(sents)) :\n",
    "    if '속보' in sents[i]:\n",
    "            check.append(sents[i])\n",
    "print(f'기존 [속보] 단어 수: {df_all2[0].apply(lambda x: re.search(\"속보\", x)is not None).sum()}')\n",
    "print(f'전처리 후 [속보] 단어 수: {len(check)}')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 [경보] 단어 수: 16068\n",
      "전처리 후 [경보] 단어 수: 15951\n"
     ]
    }
   ],
   "source": [
    "check = []\n",
    "for i in range(len(sents)) :\n",
    "    if '경보' in sents[i]:\n",
    "            check.append(sents[i])\n",
    "print(f'기존 [경보] 단어 수: {df_all2[0].apply(lambda x: re.search(\"경보\", x)is not None).sum()}')\n",
    "print(f'전처리 후 [경보] 단어 수: {len(check)}')  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테스트, 오인주의, 훈련, 점검   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 [점검] 단어 수: 1950\n",
      "전처리 후 [점검] 단어 수: 1948\n"
     ]
    }
   ],
   "source": [
    "check = []\n",
    "for i in range(len(sents)) :\n",
    "    if '점검' in sents[i]:\n",
    "            check.append(sents[i])\n",
    "print(f'기존 [점검] 단어 수: {df_all2[0].apply(lambda x: re.search(\"점검\", x)is not None).sum()}')\n",
    "print(f'전처리 후 [점검] 단어 수: {len(check)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 [훈련] 단어 수: 3099\n",
      "전처리 후 [훈련] 단어 수: 3099\n"
     ]
    }
   ],
   "source": [
    "check = []\n",
    "for i in range(len(sents)) :\n",
    "    if '훈련' in sents[i]:\n",
    "            check.append(sents[i])\n",
    "print(f'기존 [훈련] 단어 수: {df_all2[0].apply(lambda x: re.search(\"훈련\", x)is not None).sum()}')\n",
    "print(f'전처리 후 [훈련] 단어 수: {len(check)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 [오인] 단어 수: 12399\n",
      "전처리 후 [오인] 단어 수: 12188\n"
     ]
    }
   ],
   "source": [
    "check = []\n",
    "for i in range(len(sents)) :\n",
    "    if '오인' in sents[i]:\n",
    "            check.append(sents[i])\n",
    "print(f'기존 [오인] 단어 수: {df_all2[0].apply(lambda x: re.search(\"오인\", x)is not None).sum()}')\n",
    "print(f'전처리 후 [오인] 단어 수: {len(check)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 [테스트] 단어 수: 10045\n",
      "전처리 후 [테스트] 단어 수: 10045\n"
     ]
    }
   ],
   "source": [
    "check = []\n",
    "for i in range(len(sents)) :\n",
    "    if '테스트' in sents[i]:\n",
    "            check.append(sents[i])\n",
    "print(f'기존 [테스트] 단어 수: {df_all2[0].apply(lambda x: re.search(\"테스트\", x)is not None).sum()}')\n",
    "print(f'전처리 후 [테스트] 단어 수: {len(check)}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"119전처리_{start_year}-{end_year}.pickle\",\"wb\") as fw:\n",
    "    pickle.dump(sents, fw)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"119전처리_{start_year}-{end_year}.pickle\",'rb') as fr:\n",
    "    sents = pickle.load(fr)\n",
    "# df_sents = pd.DataFrame(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 분석\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer Vectorizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1368100, 69363)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "cv_sents=vect.fit_transform(sents)\n",
    "print(cv_sents.shape)\n",
    "# cv_data=cv_sents.toarray()\n",
    "# cv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1368100, 69363)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_sents =vectorizer.fit_transform(sents)\n",
    "print(tfidf_sents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778936, 10000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=0.0, analyzer='char', sublinear_tf=True, ngram_range=(1,3), max_features=10000)\n",
    "tfidf_sents =vectorizer.fit_transform(df_sents[0])\n",
    "print(tfidf_sents.shape)\n",
    "# 파라미터에 값입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec(CBOW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentences = []\n",
    "for review in sents:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터\n",
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3 # 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "print(\"Training model ....\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "속보\n",
      ": [('설비', 0.5565919876098633), ('자동', 0.5560749769210815), ('경보', 0.5363364815711975), ('소방설비', 0.5336592197418213), ('작동', 0.4983700215816498), ('인과', 0.49686312675476074), ('소방시설', 0.4892948269844055), ('점검', 0.48302412033081055), ('경보기', 0.47331950068473816), ('테스트', 0.4692937731742859)] \n",
      "\n",
      "경보\n",
      ": [('화재경보기', 0.7432859539985657), ('보가', 0.7132909297943115), ('경보기', 0.7123016715049744), ('경종', 0.6195616722106934), ('감지기', 0.6176596879959106), ('수신기', 0.603028416633606), ('알람', 0.5819022059440613), ('번쩍', 0.5372390151023865), ('속보', 0.5363364815711975), ('발신기', 0.534061074256897)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp1 = model.wv.most_similar(\"속보\")\n",
    "tmp2= model.wv.most_similar(\"경보\")\n",
    "print(f'속보\\n: {tmp1} \\n')\n",
    "print(f'경보\\n: {tmp2} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words,model, num_features):\n",
    "    feature_vector = np.zeros((num_features), dtype=np.float32)\n",
    "    num_words = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words +=1\n",
    "            feature_vector = np.add(feature_vector, model[w])\n",
    "    feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(sentence, model, num_features):\n",
    "    dataset = list()\n",
    "    for s in sentence:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "    FeatureVecs = np.stack(dataset)\n",
    "    return FeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_2544/2435305589.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  feature_vector = np.add(feature_vector, model[w])\n",
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_2544/2435305589.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  feature_vector = np.divide(feature_vector, num_words)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1368100, 300)\n"
     ]
    }
   ],
   "source": [
    "train_data_vecs = get_dataset(sentences, model, num_features)\n",
    "print(train_data_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1368100, 300)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.17214534, -0.4194798 , -0.06259897, ..., -0.3146527 ,\n",
       "        -0.31471124,  0.0989478 ],\n",
       "       [ 0.41353115,  0.58591795,  0.31289923, ..., -0.5308654 ,\n",
       "        -0.11253642, -0.37847427],\n",
       "       [ 0.5521837 , -0.76813   , -0.1612042 , ..., -0.69882536,\n",
       "        -0.11171618,  0.4713359 ],\n",
       "       ...,\n",
       "       [-0.6271333 , -0.33706298, -1.0009954 , ...,  0.6093799 ,\n",
       "        -0.6604633 ,  0.558411  ],\n",
       "       [-0.16968767,  0.04514722,  0.14016913, ...,  0.25245786,\n",
       "        -0.5659022 ,  0.27416438],\n",
       "       [-1.0406647 ,  0.03921632,  1.2199225 , ...,  0.13699155,\n",
       "         0.25819275,  0.86349905]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
